NAME=Jane
hello:
	echo Hello ${NAME}

# high level view of cluster
cluster_status: /home/dlin/make_scripts/monitoring_clusterv3.sh
	bash $<
# a lil more focused that you can get part of the job name
cluster_status2: /home/dlin/make_scripts/monitoring_clusterv2.sh
	bash $<

# replace the % with grep term to focus on certain jobs, this gets complete job path
monitor_jobs.%: /mnt/dlin/operation/outputjobs.pl
	perl $< | grep $*

# find files or dirs that were created within % minute
find_min.%:
	find . -maxdepth 1 -mmin -$*

# delete files or dirs that were created within % minute
.PHONY: remove_min.%
remove_min.%:
	find . -maxdepth 1 -mmin -$* -exec rm -vr {} \;

# # Just make sure that runid is unique among /NGS/*seq, replace % with runid
# .PHONY: seqQC_runid.%
# seqQC_runid.%: /home/dlin/make_scripts/get_sequencing_qc_DL.py
# 	python $< --runfolder /NGS/*seq/$*/ --output ./$*.csv

# This accomidates seqQC with and without noloco
seqQC_runid.%: /home/dlin/make_scripts/read_lanebarcode.py
	/home/dlin/miniconda3/envs/ambry/bin/python3.7 $< $*


# This mk cmd need an ori and a val folder in the same folder, you'll get 3 csv files: concordant, unique_clinical, unique_validation
ORI=./ori/
VAL=./val/
concord_check: /home/dlin/make_scripts/concordance_check_artf_category_tc_filtered_RD.py ${ORI} ${VAL}
	/usr/bin/python $< ${ORI} ${VAL} ./

# This need a 3 csv files, concordant, unique_clinical, unique_validation. It will give you "text.xlsx" with scraped info
scrape_concord_info: /home/dlin/make_scripts/scrape_v2.py concordant.csv unique_clinical.csv unique_validation.csv
	/home/dlin/miniconda3/envs/ambry/bin/python3.7 $<

# This generates .coverage file from BAM file. Feed it with a bed file and have "bam_lst" in the sanme folder
bam2cov_wbed.%: /home/dlin/make_scripts/wrapper4DepOCov.sh bam_lst
	bash $< $*

# This requires a cov_lst with the .coverage file paths and a bed file in the same folder
cal_cov_wbed.%: /home/dlin/make_scripts/wrapper_for_wrapper1.sh cov_lst
	bash $< $*

# Run this after csv files were generate by cal_cov_wbed, replace the % with runid
post_cov_runid.%: /home/dlin/make_scripts/post_result.R
	/home/dlin/miniconda3/bin/Rscript $< $*

# This will compile pt_cov report and write in into a xlsx file. It needs the xlsx generated by post_cov_runid.%
filter_pt_cov.%: /home/dlin/make_scripts/get_rows_20x_cov.py
	/home/dlin/miniconda3/envs/ambry/bin/python3.7 $< $*

# This requires a double_bam_lst (dedup_bam and ori_bam in two columns sparated by ",") and a bed_file=$1 , this will produce 
# picard_alignment_metrics.txt and on_target_reads.txt for each sample you can use parse_picard%ontarget.py to compile
# you can generate a sorted original bam file with sam2bam
cal_on_target_wbed.%: /home/dlin/make_scripts/wrapper_for_calc_pct_on_targetv2.sh double_bam_lst
	bash $< $*

# Run this after cal_on_target_wbed, put *on_target_reads.txt and *alignment_metrics.txt to a folder to a folder inside $PWD called ./on_target
ON_TARGET=./on_target
post_on_target: /home/dlin/make_scripts/parse_picard%ontarget.py
	/home/dlin/miniconda3/envs/ambry/bin/python3.7 $< ${ON_TARGET}

# This needs a list of run ids, deldup-noloco-org-v5-RD.sh (currently programed to work only on panel 77) 
# needs countFU-2018.py in the same folder. Script will look at NGS_deldup folder and make the calculation
# If neg control data is not wanted, rememebr to put them to a backup folder
# RUNID_LST=./runid_lst
# deldup_77: /home/dlin/make_scripts/deldup-noloco-org-v5-RD.sh ${RUNID_LST}
# 	bash $< ${RUNID_LST}

# This compiles deldup calls after test code filter, the script will need "acc_lst", a runid(or any name), 
# a folder(./deldup), any tag that you'd like. It will produce results in the working folder
ACC_LST=acc_lst
DELDUP=./deldup/
ACC_KEY=acc_key
.PHONY: get_deldup_calls_77_tag.%
get_deldup_calls_77_tag.%: /home/dlin/make_scripts/filter_deldup_patients_portal_testcode_77_original_v2.py
	/usr/bin/python $< ${ACC_LST} $* ${DELDUP} ${ACC_KEY} ./

# run this after "get_deldup_calls_77_tag.%" with deldup_count* as argv[1] and folders with *.noloco.csv as argv[2]
compile_deldupPnoloco: /home/dlin/make_scripts/compile_deldup%noloco.py
	/home/dlin/miniconda3/envs/ambry/bin/python3.7 $< $(filter-out $@,$(MAKECMDGOALS))

# This will calculate the per sample coverage from the json file, feed % with the cov_json_lst
json_cov_lst.%: /home/dlin/make_scripts/read_json.py
	/home/dlin/miniconda3/envs/ambry/bin/python3.7 $< $*

# This produces del and reupload command for panel 77 on s70
gen_purge_upload_77_runid.%: /home/dlin/make_scripts/77_gen_purge_s70.sh
	bash $< $*

# This produces del and reupload command for panel 77 on s70
gen_purge_upload_76_runid.%: /home/dlin/make_scripts/76_gen_purge_s70.sh
	bash $< $*

# This produces del and reupload command for panel 77 on s70
gen_purge_upload_73_runid.%: /home/dlin/make_scripts/73_gen_purge_s70.sh
	bash $< $*

# This produces del and reupload command for panel 77 on s70
gen_purge_upload_72_runid.%: /home/dlin/make_scripts/72_gen_purge_s70.sh
	bash $< $*

# This counts noloco feed it with a list of noloco.csv files
count_noloco.%: /home/dlin/make_scripts/count_noloco_csv_v2.py
	/home/dlin/miniconda3/envs/ambry/bin/python3.7 $< $*

# This submits noloco10x job with selected noloco.txt file, it needs a sample_lst, and all the sample .coverage file, and a noloco.txt file
noloco_super10x.%: /home/dlin/make_scripts/wrapper_noloco_pipeline10X.sh sample_lst
	bash $< $*

# This submits noloco20x job with selected noloco.txt file, it needs a sample_lst, and all the sample .coverage file, and a noloco.txt file
noloco_super20x.%: /home/dlin/make_scripts/wrapper_noloco_pipeline20X.sh sample_lst
	bash $< $*
# Feed it with noloco.txt file as $1 and have sample_lst present in working dir
noloco_super100x.%: /home/dlin/make_scripts/wrapper_noloco_pipeline100X.sh sample_lst
	bash $< $*

# This will intersect the vcf with bed to create .recode.vcf, got position_str, inner join with .vcf.out and compile annotated variants
# It need a bed file[1], vcf_lst, vcf_out_lst in the same folder
filter_vcf_get_variant_wbed.%: /home/dlin/make_scripts/vcf_bed_process.sh vcf_lst vcf_out_lst
	bash $< $*

# this will calculate avg, std, t and p value for noloco# from two noloco_csv_lsts,  $(filter-out $@,$(MAKECMDGOALS)) is helping to put two 
# positional args. it should be run like python noloco_t_test.py noloco_csv_lst1 noloco_csv_lst2
noloco_t_test: /home/dlin/make_scripts/noloco_t_test.py
	/home/dlin/miniconda3/envs/ambry/bin/python3.7 $< $(filter-out $@,$(MAKECMDGOALS))

# needs dedup_bam_lst for cat and picard_interval_lst as $1, generates sample.hs_metrics that has per base coverage info
# pay attention to the reference genome for exome is different from others
picard_hsmetrcs_winterval_lst.%: /home/dlin/make_scripts/wrapper_for_picard_hs_metrics.sh hs_metrics_bam_lst
	bash $< $*
	
# needs dedup_bam_lst as $1, this will generate pdf files with read fragment size for each sample
picard_insert_size_wbam_lst.%: /home/dlin/make_scripts/wrapper_for_picard_insert_size.sh
	bash $< $*

# needs original_bam_lst before deduplication as $1, this will generate pcr dup rate for each sample
# panel 77 does not have ori_bam, only dedup_bam were produced. We have to make ori_bam from sam
picard_complexity_wbam_lst.%: /home/dlin/make_scripts/wrapper_for_picard_complexity.sh
	bash $< $*